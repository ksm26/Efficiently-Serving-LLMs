# ğŸš€ [Efficiently Serving Large Language Models](https://www.deeplearning.ai/short-courses/efficiently-serving-llms/)

ğŸ’» Welcome to the "Efficiently Serving Large Language Models" course! Instructed by Travis Addair, Co-Founder and CTO at Predibase, this course will deepen your understanding of serving LLM applications efficiently.

**Course Website**: ğŸ“š[deeplearning.ai](https://www.deeplearning.ai/short-courses/efficiently-serving-llms/)

## Course Summary
In this course, you'll delve into the optimization techniques necessary to efficiently serve Large Language Models (LLMs) to a large number of users. Here's what you can expect to learn and experience:

1. ğŸ¤– **Auto-Regressive Models**: Understand how auto-regressive large language models generate text token by token.
2. ğŸ’» **LLM Inference Stack**: Implement foundational elements of a modern LLM inference stack, including KV caching, continuous batching, and model quantization.
3. ğŸ› ï¸ **LoRA Adapters**: Explore the details of how Low Rank Adapters (LoRA) work and how batching techniques allow different LoRA adapters to be served to multiple customers simultaneously.
4. ğŸš€ **Hands-On Experience**: Get hands-on with Predibaseâ€™s LoRAX framework inference server to see optimization techniques in action.

## Key Points
- ğŸ” Learn techniques like KV caching to speed up text generation in Large Language Models (LLMs).
- ğŸ’» Write code to efficiently serve LLM applications to a large number of users while considering performance trade-offs.
- ğŸ› ï¸ Explore the fundamentals of Low Rank Adapters (LoRA) and how Predibase implements them in the LoRAX framework inference server.

## About the Instructor
ğŸŒŸ **Travis Addair** is the Co-Founder and CTO at Predibase, bringing extensive expertise to guide you through efficiently serving Large Language Models (LLMs).

ğŸ”— To enroll in the course or for further information, visit [deeplearning.ai](https://www.deeplearning.ai/short-courses/).
